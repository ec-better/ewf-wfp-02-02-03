{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ewf-wfp-02-02-03 - Snow Cover Characterization Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snow Cover Characterization Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'Snow Cover Characterization Anomalies'),\n",
    "                ('abstract', 'Snow Cover Characterization Anomalies'),\n",
    "                ('id', 'ewf-wfp-02-02-03')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexAggCat = dict([('id', 'indexAggCat'),\n",
    "                    ('value', 'better-wfp-02-02-01'),\n",
    "                    ('title', 'indexAggCat'),\n",
    "                    ('abstract', 'index to access catalog of aggregated land surface temperature time series'),\n",
    "                    ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikeyAggCat = dict([('id', 'apikeyAggCat'),\n",
    "                     ('value', ''),\n",
    "                     ('title', 'apikeyAggCat'),\n",
    "                     ('abstract', 'apikey to access indexAggCat catalog'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexLtaCat = dict([('id', 'indexLtaCat'),\n",
    "                    ('value', 'better-wfp-02-02-02'),\n",
    "                    ('title', 'indexLtaCat'),\n",
    "                    ('abstract', 'index to access catalog of aggregated land surface temperature time series'),\n",
    "                    ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikeyLtaCat = dict([('id', 'apikeyLtaCat'),\n",
    "                     ('value', ''),\n",
    "                     ('title', 'apikeyLtaCat'),\n",
    "                     ('abstract', 'apikey to access indexAggCat catalog'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the MDOIS stack of products' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_identifiers = ('2FBBF58C17C3124FDDCFF4BC8E7C985FFFD17DBE', '0C63C452BB93FCFAF834FC1724E00364B84B92A7')\n",
    "input_identifiers = ('4B4BF219C839E7428A0828B3AB0BCA82311D8511','04843237511A009385116E9C13DCDB2A09B33A97')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the MODIS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/better-wfp-02-02-01/search?uid={}'.format(input_identifiers[0]), 'https://catalog.terradue.com/better-wfp-02-02-02/search?uid={}'.format(input_identifiers[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/dev/ewf-wfp-02-02-03/src/main/app-resources/notebook/libexec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = 'temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import cioppy\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import pdb\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_metadata (input_refs):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        if 'LTA_' in product_ref:\n",
    "            apikey = apikeyLtaCat\n",
    "            indexCat = indexLtaCat\n",
    "        else:\n",
    "            apikey = apikeyAggCat\n",
    "            indexCat = indexAggCat\n",
    "        \n",
    "        # since the search is by identifier \n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,params =[],output_fields='self,identifier,startdate,enclosure,title,startdate,enddate,wkt',creds='{}:{}'.format(indexCat['value'],apikey['value']))[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = gpd.GeoDataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    return input_metadata\n",
    "\n",
    "def rm_cfolder(folder):\n",
    "    #folder = '/path/to/folder'\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "    \n",
    "def get_metadata(filepath):\n",
    "        \n",
    "    #pdb.set_trace()\n",
    "    ds = gdal.Open(filepath)\n",
    "    projection = ds.GetProjection()\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    no_data_value = ds.GetRasterBand(1).GetNoDataValue()\n",
    "    data_type = ds.GetRasterBand(1).DataType\n",
    "    return projection, geotransform, no_data_value, data_type\n",
    "\n",
    "\n",
    "def get_matrix_list(image_list):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "        \n",
    "        print(type(product_array))\n",
    "    return mat_list\n",
    "\n",
    "\n",
    "def calc_anomaly(agg_file, LTA_file):\n",
    "    \n",
    "    \n",
    "    if agg_file and LTA_file:\n",
    "        \n",
    "        \n",
    "        projection, geotransform, no_data_value, data_type = get_metadata(agg_file)\n",
    "        \n",
    "        \n",
    "        agg_and_LTA = get_matrix_list([agg_file, LTA_file])\n",
    "        print('Aggregation and LTA converted to matrices')\n",
    "        \n",
    "    \n",
    "        if 'uint' in str(agg_and_LTA[0].dtype) or 'uint' in str(agg_and_LTA[1].dtype):\n",
    "            agg_and_LTA[0] = np.int16(agg_and_LTA[0])\n",
    "            agg_and_LTA[1] = np.int16(agg_and_LTA[1])\n",
    "            \n",
    "        anomaly_values = agg_and_LTA[0] - agg_and_LTA[1]\n",
    "        \n",
    "        anomaly_values[np.logical_or(agg_and_LTA[0] == no_data_value, agg_and_LTA[1] == no_data_value)] = no_data_value\n",
    "\n",
    "        \n",
    "        return anomaly_values, projection, geotransform, no_data_value, data_type\n",
    "    \n",
    "    else:\n",
    "        return None, None, None\n",
    "   \n",
    "\n",
    "\n",
    "def get_anom_dates (agg, LTA, first_year_agg, no_data_value):\n",
    "\n",
    "    if agg == no_data_value or LTA == no_data_value:\n",
    "        return no_data_value\n",
    "    \n",
    "    aug_1st = dt.date(first_year_agg, 8, 1).timetuple().tm_yday # 1st aug\n",
    "    ndays_year = dt.date(first_year_agg, 12, 31).timetuple().tm_yday ## days of the first year\n",
    "\n",
    "    if agg < aug_1st and LTA < aug_1st:\n",
    "    \n",
    "        anom = agg - LTA\n",
    "    \n",
    "    elif agg >= aug_1st and LTA >= aug_1st:\n",
    "    \n",
    "        anom = agg - LTA\n",
    "    \n",
    "    elif agg < aug_1st and LTA >= aug_1st:\n",
    "    \n",
    "        d1 = ndays_year - LTA\n",
    "        d2 = agg\n",
    "        anom = d1 + d2\n",
    "    \n",
    "    elif agg >= aug_1st and LTA < aug_1st:\n",
    "    \n",
    "        d1 = ndays_year - agg\n",
    "        d2 = LTA\n",
    "        anom = -(d1 + d2)\n",
    "    \n",
    "    return anom\n",
    "\n",
    "\n",
    "def calc_anomaly_dates(agg_file, LTA_file, first_year_agg):\n",
    "    \n",
    "    if agg_file and LTA_file:\n",
    "        \n",
    "        \n",
    "        agg_and_LTA = get_matrix_list([agg_file, LTA_file])\n",
    "        print('Aggregation and LTA converted to matrices')\n",
    "        \n",
    "        projection, geotransform, no_data_value, data_type = get_metadata(agg_file)\n",
    "        \n",
    "        \n",
    "        anom_dates_calculator = np.vectorize(get_anom_dates)\n",
    "            \n",
    "        anomaly_values = anom_dates_calculator(agg_and_LTA[0], agg_and_LTA[1], first_year_agg, no_data_value)\n",
    "        \n",
    "        \n",
    "        return anomaly_values, projection, geotransform, no_data_value, data_type\n",
    "    \n",
    "    else:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    \n",
    "\n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    if mask is not None and mask is not 0:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "    \n",
    "\n",
    "def write_anomaly_output(anomaly, output_folder, product_name, first_date, last_date, lta_start_year, lta_end_year, mask_no_value, roi_name, projection, geo_transform, no_data_value):\n",
    "    \n",
    "    filename = os.path.join(output_folder, '_'.join(['Anomaly', product_name, roi_name, first_date.split('_')[0], last_date.split('_')[0], 'LTA', str(lta_start_year), str(lta_end_year)]) + '.tif')\n",
    "    \n",
    "    write_output_image(filename, anomaly, 'GTiff', gdal.GDT_Float32, mask_no_value, projection, geo_transform, no_data_value)\n",
    "    return filename\n",
    "\n",
    "def get_formatted_date(date_str):\n",
    "    date = datetime.datetime.strftime(date_str, '%Y-%m-%dT00:00:00Z')\n",
    "    return dat\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders\n",
    "#if not os.path.isdir(data_path):\n",
    "#    os.mkdir(data_path)\n",
    "\n",
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metadata = get_input_metadata(input_references)\n",
    "input_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(input_identifiers, str):\n",
    "    input_identifiers = [input_identifiers]\n",
    "\n",
    "\n",
    "enclosure = list(input_metadata['enclosure'])\n",
    "filepath_agg = os.path.join(data_path, os.path.basename(enclosure[0]).split('?')[0]) \n",
    "filepath_LTA = os.path.join(data_path, os.path.basename(enclosure[1]).split('?')[0])\n",
    "\n",
    "# list of files\n",
    "print(filepath_agg)\n",
    "print(filepath_LTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata from filenames\n",
    "\n",
    "file_name_elements = os.path.basename(filepath_agg).split('.')[0].split('_')\n",
    "print(file_name_elements)\n",
    "\n",
    "\n",
    "\n",
    "first_date = file_name_elements[-2]\n",
    "last_date = file_name_elements[-1]\n",
    "agg_type = '_'.join(file_name_elements[-5:-3])\n",
    "name_of_region = file_name_elements[-3]\n",
    "\n",
    "\n",
    "file_name_elements = os.path.basename(filepath_LTA).split('.')[0].split('_')\n",
    "\n",
    "\n",
    "lta_start_year = file_name_elements[-2]\n",
    "lta_end_year = file_name_elements[-1]\n",
    "\n",
    "prod_type = file_name_elements[1] + '_' + file_name_elements[-4]\n",
    "\n",
    "\n",
    "print(filepath_agg)\n",
    "print(filepath_LTA)\n",
    "\n",
    "if 'SeasonDate' in prod_type:\n",
    "    anomaly_values, projection, geotransform, no_data_value, data_type = calc_anomaly_dates(filepath_agg, filepath_LTA, int(first_date))\n",
    "else:\n",
    "    anomaly_values, projection, geotransform, no_data_value, data_type = calc_anomaly(filepath_agg, filepath_LTA)\n",
    "\n",
    "\n",
    "filename = write_anomaly_output(anomaly_values, output_folder, prod_type, first_date, last_date, lta_start_year, lta_end_year, None, name_of_region, projection, geotransform, no_data_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = input_metadata['startdate'].iloc[0].strftime('%Y-%m-%dT00:00:00Z')\n",
    "enddate = input_metadata['enddate'].iloc[0].strftime('%Y-%m-%dT00:00:00Z')\n",
    "wkt = input_metadata['wkt'].iloc[0]\n",
    "\n",
    "write_properties_file(filename, startdate, enddate, wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_results:\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(anomaly_values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporay files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cfolder(temp_folder)\n",
    "\n",
    "os.rmdir(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
