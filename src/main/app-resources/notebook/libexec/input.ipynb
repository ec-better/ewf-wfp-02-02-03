{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ewf-wfp-02-02-03 - Snow Cover Characterization Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snow Cover Characterization Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'Snow Cover Characterization Anomalies'),\n",
    "                ('abstract', 'Snow Cover Characterization Anomalies'),\n",
    "                ('id', 'ewf-wfp-02-02-03')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "#                     ('value', 'CentralAsia'),\n",
    "#                     ('title', 'Name of Region'),\n",
    "#                     ('abstract', 'Name of the region of interest'),\n",
    "#                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexAggCat = dict([('id', 'indexAggCat'),\n",
    "             ('value', 'better-wfp-02-02-01'),\n",
    "             ('title', 'indexAggCat'),\n",
    "             ('abstract', 'index to access catalog of aggregated land surface temperature time series'),\n",
    "             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikeyAggCat = dict([('id', 'apikeyAggCat'),\n",
    "                ('value', ''),\n",
    "                ('title', 'apikeyAggCat'),\n",
    "                ('abstract', 'apikey to access indexAggCat catalog'),\n",
    "                ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexLtaCat = dict([('id', 'indexLtaCat'),\n",
    "             ('value', 'better-wfp-02-02-02'),\n",
    "             ('title', 'indexLtaCat'),\n",
    "             ('abstract', 'index to access catalog of aggregated land surface temperature time series'),\n",
    "             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikeyLtaCat = dict([('id', 'apikeyLtaCat'),\n",
    "                ('value', ''),\n",
    "                ('title', 'apikeyLtaCat'),\n",
    "                ('abstract', 'apikey to access indexAggCat catalog'),\n",
    "                ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the MDOIS stack of products' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2015, 2016, 2017\n",
    "input_identifiers = ('26DDF6E2F0A9653EE4878D934A9EF222EA7EB245', 'B7E4C6A85DE3478661E8B5489D84DA2BC0C0EF23')\n",
    "\n",
    "#, 'LST_SouthernAfrica_N3_maxvalues_2015-01-01_2015-01-21.tif', 'LST_SouthernAfrica_N3_maxvalues_2016-01-01_2016-01-21.tif', 'LST_SouthernAfrica_N3_maxvalues_2017-01-01_2017-01-21.tif'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the MODIS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/better-wfp-02-02-01/search?uid={}'.format(input_identifiers[0]), 'https://catalog.terradue.com/better-wfp-02-02-02/search?uid={}'.format(input_identifiers[1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/ewf-wfp-02-02-03/src/main/app-resources/notebook/libexec/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = 'temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import cioppy\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import pdb\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_metadata (input_refs):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        if 'LTA_' in product_ref:\n",
    "            apikey = apikeyLtaCat\n",
    "            indexCat = indexLtaCat\n",
    "        else:\n",
    "            apikey = apikeyAggCat\n",
    "            indexCat = indexAggCat\n",
    "        \n",
    "        # since the search is by identifier \n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,params =[],output_fields='self,identifier,startdate,enclosure,title,startdate,enddate,wkt',creds='{}:{}'.format(indexCat['value'],apikey['value']))[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = gpd.GeoDataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    return input_metadata\n",
    "\n",
    "def rm_cfolder(folder):\n",
    "    #folder = '/path/to/folder'\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e) \n",
    "    \n",
    "def get_metadata(filepath):\n",
    "        \n",
    "    #pdb.set_trace()\n",
    "    ds = gdal.Open(filepath)\n",
    "    projection = ds.GetProjection()\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    no_data_value = ds.GetRasterBand(1).GetNoDataValue()\n",
    "    data_type = ds.GetRasterBand(1).DataType\n",
    "    return projection, geotransform, no_data_value, data_type\n",
    "\n",
    "\n",
    "def get_matrix_list(image_list):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "        \n",
    "        print(type(product_array))\n",
    "    return mat_list\n",
    "\n",
    "\n",
    "def calc_anomaly(agg_file, LTA_file):\n",
    "    \n",
    "    \n",
    "    #file_list = []\n",
    "    #if not os.path.isdir('tmp_data'):\n",
    "    #    os.mkdir('tmp_data')\n",
    "    #for enclosure in dataframe['enclosure'].tolist():\n",
    "    #    filepath = 'tmp_data/' + os.path.basename(enclosure)\n",
    "    #    status = get_product(enclosure, filepath)\n",
    "    #    if status == 200:\n",
    "    #        file_list.append(filepath)\n",
    "    #print(file_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if agg_file and LTA_file:\n",
    "        \n",
    "        \n",
    "        agg_and_LTA = get_matrix_list([agg_file, LTA_file])\n",
    "        print('Aggregation and LTA converted to matrices')\n",
    "        \n",
    "        print(agg_and_LTA[0].dtype)\n",
    "        print(agg_and_LTA[1].dtype)\n",
    "        \n",
    "        #anomaly_values = np.divide(agg_and_LTA[0] * 1.0, agg_and_LTA[1] * 1.0)\n",
    "        \n",
    "        #anomaly_values = np.divide(agg_and_LTA[0] * 1.0, agg_and_LTA[1] * 1.0, out=np.zeros_like(agg_and_LTA[0] * 1.0), where=agg_and_LTA[1]!=0)\n",
    "        if 'uint' in str(agg_and_LTA[0].dtype) or 'uint' in str(agg_and_LTA[1].dtype):\n",
    "            agg_and_LTA[0] = np.int16(agg_and_LTA[0])\n",
    "            agg_and_LTA[1] = np.int16(agg_and_LTA[1])\n",
    "            \n",
    "        anomaly_values = agg_and_LTA[0] - agg_and_LTA[1]\n",
    "        \n",
    "        anomaly_values[(agg_and_LTA[0] == 0)] = -9999\n",
    "        anomaly_values[(agg_and_LTA[1] == 0)] = -9999\n",
    "        \n",
    "        print(anomaly_values.dtype)\n",
    "        \n",
    "        projection, geotransform, no_data_value, data_type = get_metadata(agg_file)\n",
    "        \n",
    "        no_data_value = -9999\n",
    "        \n",
    "        \n",
    "        \n",
    "        #for file_ in file_list:\n",
    "        #    os.remove(file_)\n",
    "        \n",
    "        return anomaly_values, projection, geotransform, no_data_value, data_type\n",
    "    \n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    if mask is not None and mask is not 0:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "    \n",
    "\n",
    "def write_anomaly_output(anomaly, output_folder, product_name, first_date, last_date, lta_start_year, lta_end_year, mask_no_value, roi_name, projection, geo_transform, no_data_value):\n",
    "    #image_number = (datetime.strptime(last_date, '%Y-%m-%d') - datetime.strptime(first_date, '%Y-%m-%d')).days\n",
    "    \n",
    "    \n",
    "    #filename =  output_folder + '/' + product_name + '_Anomaly_' + roi_name + '_N' + str(N_value) + '_' + aggregation + '_' + first_date + '_' + last_date + '_LTA' + str(lta_start_year) + '_' + str(lta_end_year) + '.tif'\n",
    "    \n",
    "    #snow_ndays_Anomaly_CentralAsia_2015-08-05_2016-07-27_LTA2015_2017.tif\n",
    "    #Anomaly_SCNDays_CentralAsia_2015_2016_LTA_2015_2017.tif\n",
    "    \n",
    "    filename = os.path.join(output_folder, '_'.join(['Anomaly', product_name, roi_name, first_date.split('_')[0], last_date.split('_')[0], 'LTA', str(lta_start_year), str(lta_end_year)]) + '.tif')\n",
    "    \n",
    "    write_output_image(filename, anomaly, 'GTiff', gdal.GDT_Float32, mask_no_value, projection, geo_transform, no_data_value)\n",
    "    return filename\n",
    "\n",
    "def get_formatted_date(date_str):\n",
    "    date = datetime.datetime.strftime(date_str, '%Y-%m-%dT00:00:00Z')\n",
    "    return dat\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    #first_date = get_formatted_date(first_date)\n",
    "    #last_date = get_formatted_date(last_date)\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders\n",
    "#if not os.path.isdir(data_path):\n",
    "#    os.mkdir(data_path)\n",
    "\n",
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           enclosure    enddate  \\\n",
      "0  https://store.terradue.com/better-wfp-02-02-01... 2016-07-27   \n",
      "1  https://store.terradue.com/better-wfp-02-02-02... 2017-07-28   \n",
      "\n",
      "                                 identifier  \\\n",
      "0  26DDF6E2F0A9653EE4878D934A9EF222EA7EB245   \n",
      "1  B7E4C6A85DE3478661E8B5489D84DA2BC0C0EF23   \n",
      "\n",
      "                                                self  startdate  \\\n",
      "0  https://catalog.terradue.com/better-wfp-02-02-... 2015-08-05   \n",
      "1  https://catalog.terradue.com/better-wfp-02-02-... 2015-08-05   \n",
      "\n",
      "                                            title  \\\n",
      "0      Output SCNDays_MTerra_h23v05_2015_2016.tif   \n",
      "1  Output LTA_MTerra_SCNDays_h23v05_2015_2017.tif   \n",
      "\n",
      "                                                 wkt  \n",
      "0  POLYGON((57.5135441530014 29.8885828072669,65....  \n",
      "1  POLYGON((57.5135441530014 29.8885828072669,65....  \n"
     ]
    }
   ],
   "source": [
    "input_metadata = get_input_metadata(input_references)\n",
    "#N = nv\n",
    "print(input_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/ewf-wfp-02-02-03/src/main/app-resources/notebook/libexec/data/SCNDays_MTerra_h23v05_2015_2016.tif\n",
      "/workspace/ewf-wfp-02-02-03/src/main/app-resources/notebook/libexec/data/LTA_MTerra_SCNDays_h23v05_2015_2017.tif\n",
      "['SCNDays', 'MTerra', 'h23v05', '2015', '2016']\n",
      "/workspace/ewf-wfp-02-02-03/src/main/app-resources/notebook/libexec/data/SCNDays_MTerra_h23v05_2015_2016.tif\n",
      "/workspace/ewf-wfp-02-02-03/src/main/app-resources/notebook/libexec/data/LTA_MTerra_SCNDays_h23v05_2015_2017.tif\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "Aggregation and LTA converted to matrices\n",
      "uint16\n",
      "uint16\n",
      "int16\n"
     ]
    }
   ],
   "source": [
    "#print('hello')\n",
    "#pdb.set_trace()\n",
    "if isinstance(input_identifiers, str):\n",
    "    input_identifiers = [input_identifiers]\n",
    "\n",
    "#region_of_interest = regionOfInterest['value']\n",
    "#name_of_region = nameOfRegion['value']\n",
    "\n",
    "#filepath_agg = os.path.join(data_path, input_references[0].split('/')[-1])\n",
    "#filepath_LTA = os.path.join(data_path, input_references[1].split('/')[-1])\n",
    "enclosure = list(input_metadata['enclosure'])\n",
    "filepath_agg = os.path.join(data_path, os.path.basename(enclosure[0]).split('?')[0]) \n",
    "filepath_LTA = os.path.join(data_path, os.path.basename(enclosure[1]).split('?')[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#filepath_agg = data_path + '/' + input_identifiers[0]\n",
    "#filepath_LTA = data_path + '/' + input_identifiers[1]\n",
    "\n",
    "# list of files\n",
    "print(filepath_agg)\n",
    "print(filepath_LTA)\n",
    "\n",
    "\n",
    "\n",
    "# get metadata from filenames\n",
    "\n",
    "file_name_elements = os.path.basename(filepath_agg).split('.')[0].split('_')\n",
    "#['LST', 'SouthernAfrica', 'N3', 'averages', '2015-01-01', '2015-01-21']\n",
    "#['snow', 'ndays', 'CentralAsia', '2015-08-05', '2016-07-27']\n",
    "print(file_name_elements)\n",
    "\n",
    "\n",
    "\n",
    "first_date = file_name_elements[-2]\n",
    "last_date = file_name_elements[-1]\n",
    "agg_type = '_'.join(file_name_elements[-5:-3])\n",
    "name_of_region = file_name_elements[-3]\n",
    "#Nn = file_name_elements[-4]\n",
    "\n",
    "\n",
    "file_name_elements = os.path.basename(filepath_LTA).split('.')[0].split('_')\n",
    "#['LTA', 'LST', 'SouthernAfrica', 'N3', 'averages', '1-1', '1-21', '2015', '2017']\n",
    "\n",
    "lta_start_year = file_name_elements[-2]\n",
    "lta_end_year = file_name_elements[-1]\n",
    "\n",
    "prod_type = file_name_elements[-4]\n",
    "#Nn_LTA = file_name_elements[-6]\n",
    "\n",
    "#['LTA', 'snow', 'ndays', 'CentralAsia', '8-5', '7-27', '2015', '2017']\n",
    "\n",
    "\n",
    "print(filepath_agg)\n",
    "print(filepath_LTA)\n",
    "\n",
    "\n",
    "anomaly_values, projection, geotransform, no_data_value, data_type = calc_anomaly(filepath_agg, filepath_LTA)\n",
    "\n",
    "\n",
    "filename = write_anomaly_output(anomaly_values, output_folder, prod_type, first_date, last_date, lta_start_year, lta_end_year, None, name_of_region, projection, geotransform, no_data_value)\n",
    "\n",
    "startdate = input_metadata['startdate'].iloc[0].strftime('%Y-%m-%dT00:00:00Z')\n",
    "enddate = input_metadata['enddate'].iloc[0].strftime('%Y-%m-%dT00:00:00Z')\n",
    "wkt = input_metadata['wkt'].iloc[0]\n",
    "\n",
    "write_properties_file(filename, startdate, enddate, wkt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_results:\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(anomaly_values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_results:\n",
    "\n",
    "    import rasterio\n",
    "\n",
    "    from rasterio.plot import show\n",
    "\n",
    "    # Load data\n",
    "    #raster = output_folder + '/' + 'LST_Anomaly_SouthernAfrica_N3_averages_2015-01-01_2015-01-21_LTA2015_2017.tif'\n",
    "    raster = filename\n",
    "    data = rasterio.open(raster)\n",
    "\n",
    "    show(data)\n",
    "\n",
    "    #data = georasters.from_file(raster)\n",
    "\n",
    "    # Plot data\n",
    "    #data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporay files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cfolder(temp_folder)\n",
    "\n",
    "os.rmdir(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
